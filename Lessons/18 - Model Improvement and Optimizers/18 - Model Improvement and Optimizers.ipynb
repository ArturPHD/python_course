{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 18: Model Improvement - Custom Classes and Optimizers\n",
    "\n",
    "In the previous lesson, we trained a model using standard settings (SGD, Learning Rate 0.01). It worked, but can we do better?\n",
    "\n",
    "Today we dive deeper into the \"brain\" of the training process:\n",
    "1.  **Advanced `nn.Module`**: Building flexible, reusable model classes.\n",
    "2.  **Optimizers**: Why `Adam` is often better than `SGD`.\n",
    "3.  **Learning Rate**: The most important hyperparameter and how to tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Videos\n",
    "\n",
    "Understanding optimizers is much easier with visuals. Check these out:\n",
    "\n",
    "* **Optimizers Explained (Vizualization):** A great visual comparison of how SGD, Momentum, and Adam navigate the \"loss landscape\".\n",
    "    * [https://www.youtube.com/watch?v=mdKjMPmcWjY](https://www.youtube.com/watch?v=mdKjMPmcWjY)\n",
    "\n",
    "* **The Learning Rate (Andrew Ng):** A classic explanation of how step size affects convergence.\n",
    "    * [https://www.youtube.com/watch?v=4qJaSmvhxi8](https://www.youtube.com/watch?v=4qJaSmvhxi8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "We use the same setup as before, importing our reusable `data_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Fix import path for data_loader ---\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "data_loader_path = os.path.join(parent_dir, '16 - Datasets')\n",
    "if data_loader_path not in sys.path:\n",
    "    sys.path.append(data_loader_path)\n",
    "\n",
    "from data_loader import get_mnist_loaders\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# --- Constants ---\n",
    "BATCH_SIZE = 64\n",
    "INPUT_SIZE = 784\n",
    "NUM_CLASSES = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Data\n",
    "train_loader, val_loader, test_loader = get_mnist_loaders(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Improving the Model Architecture (`nn.Module`)\n",
    "\n",
    "We can make our model class more flexible. Instead of hardcoding layers, we can pass configuration parameters to `__init__`.\n",
    "\n",
    "We will also introduce a new layer type: **Dropout**.\n",
    "\n",
    "### What is Dropout?\n",
    "Dropout is a regularization technique to prevent **Overfitting**.\n",
    "* During training, it randomly \"turns off\" (zeros out) a percentage of neurons (e.g., 20%).\n",
    "* This forces the network not to rely too heavily on any single neuron. It makes the network more robust, like a team where every member can handle tasks if someone is sick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int): Size of input vector (784)\n",
    "            hidden_sizes (list): A list of integers, e.g., [128, 64]\n",
    "            output_size (int): Number of classes (10)\n",
    "            dropout_rate (float): Probability of an element to be zeroed.\n",
    "        \"\"\"\n",
    "        super(FlexibleNN, self).__init__()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # We can build layers dynamically using a list\n",
    "        layers = []\n",
    "        \n",
    "        # Input Layer -> First Hidden Layer\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(p=dropout_rate))\n",
    "            prev_size = size\n",
    "        \n",
    "        # Last Hidden Layer -> Output Layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        \n",
    "        # Wrap the list in nn.Sequential\n",
    "        self.model_stack = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.model_stack(x)\n",
    "        return logits\n",
    "\n",
    "# Example: Creating a deeper model easily\n",
    "model = FlexibleNN(784, [256, 128, 64], 10, dropout_rate=0.2).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimizers: SGD vs. Adam\n",
    "\n",
    "The optimizer is the algorithm that updates the weights based on the gradients.\n",
    "\n",
    "### SGD (Stochastic Gradient Descent)\n",
    "* **Analogy:** You are walking down a misty mountain. You look at your feet, see which way is down, and take a fixed-size step. \n",
    "* **Pros:** Simple, well-understood.\n",
    "* **Cons:** Can get stuck in local valleys. If the slope is steep, it might overshoot. If the slope is flat, it moves very slowly.\n",
    "\n",
    "### Adam (Adaptive Moment Estimation)\n",
    "* **Analogy:** You are a heavy ball rolling down the mountain. \n",
    "    1.  **Momentum:** If you are moving fast in one direction, you keep going (even if the slope changes slightly).\n",
    "    2.  **Adaptivity:** If you haven't moved much in a certain direction recently, you take bigger steps. If you are moving a lot, you take smaller, more careful steps.\n",
    "* **Pros:** Generally converges *much* faster and requires less tuning of the learning rate.\n",
    "* **Cons:** More complex math (but PyTorch handles it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Learning Rate (LR)\n",
    "\n",
    "This is the single most important setting.\n",
    "\n",
    "* **Low LR (e.g., 0.0001):** The model learns very slowly. It might take 100 epochs to get anywhere.\n",
    "* **High LR (e.g., 0.1 or 1.0):** The model makes huge changes. It might jump *over* the optimal solution and never settle down. The loss might even explode to Infinity.\n",
    "* **Good LR:** Fast convergence initially, settling into the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experimentation Time\n",
    "\n",
    "Let's define a reusable training function so we can easily run multiple experiments and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(name, model, optimizer, num_epochs=5):\n",
    "    print(f\"\\n--- Starting Experiment: {name} ---\")\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    val_accuracies = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        acc = 100 * correct / total\n",
    "        val_accuracies.append(acc)\n",
    "        print(f\"Epoch {epoch+1}: Val Acc = {acc:.2f}%\")\n",
    "    \n",
    "    print(f\"Finished in {time.time() - start_time:.2f} seconds\")\n",
    "    return val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: SGD (The Baseline)\n",
    "Standard parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd = FlexibleNN(784, [128, 64], 10).to(device)\n",
    "opt_sgd = optim.SGD(model_sgd.parameters(), lr=0.01)\n",
    "\n",
    "results_sgd = run_experiment(\"SGD (lr=0.01)\", model_sgd, opt_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Adam (The Modern Standard)\n",
    "Same Learning Rate, just changing the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam = FlexibleNN(784, [128, 64], 10).to(device)\n",
    "opt_adam = optim.Adam(model_adam.parameters(), lr=0.01)\n",
    "\n",
    "results_adam = run_experiment(\"Adam (lr=0.01)\", model_adam, opt_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Adam with Lower Learning Rate\n",
    "Adam is more aggressive. Sometimes 0.01 is too high. Let's try 0.001 (the default for Adam in many libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam_low = FlexibleNN(784, [128, 64], 10).to(device)\n",
    "opt_adam_low = optim.Adam(model_adam_low.parameters(), lr=0.001)\n",
    "\n",
    "results_adam_low = run_experiment(\"Adam (lr=0.001)\", model_adam_low, opt_adam_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Comparison\n",
    "\n",
    "Let's plot the results side-by-side to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_sgd, label='SGD (lr=0.01)', marker='o')\n",
    "plt.plot(results_adam, label='Adam (lr=0.01)', marker='o')\n",
    "plt.plot(results_adam_low, label='Adam (lr=0.001)', marker='o')\n",
    "\n",
    "plt.title('Optimizer Comparison: Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You should likely see that:\n",
    "1.  **Adam** converges faster than **SGD**.\n",
    "2.  **Adam with high LR (0.01)** might be unstable or erratic.\n",
    "3.  **Adam with low LR (0.001)** usually gives the smoothest and best results for this problem.\n",
    "\n",
    "This is why **Hyperparameter Tuning** is so important. Just changing one line of code can boost accuracy significantly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}