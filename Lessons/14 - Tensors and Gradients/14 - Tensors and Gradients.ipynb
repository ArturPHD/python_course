{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 14: Introduction to PyTorch - Tensors and Gradients\n",
    "\n",
    "Welcome to the third and final part of our course. We've built our foundation in Python and Data Analysis. Now, we enter the world of Deep Learning.\n",
    "\n",
    "Our tool for this journey is **PyTorch**, one of the world's leading deep learning frameworks. This lesson is fundamental. We will learn about the one and only building block of PyTorch: the **Tensor**.\n",
    "\n",
    "We will cover:\n",
    "1.  What PyTorch is and why we use it.\n",
    "2.  What a **Tensor** is (Scalars, Vectors, Matrices, etc.).\n",
    "3.  The deep relationship between **PyTorch Tensors and NumPy Arrays**.\n",
    "4.  Why we need Tensors: **The Simple Perceptron & Matrix Multiplication**.\n",
    "5.  The first piece of PyTorch magic: **Autograd (Automatic Gradients)**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is PyTorch?\n",
    "\n",
    "PyTorch is an open-source machine learning library developed primarily by Facebook's AI Research lab (FAIR). At its core, it provides two fundamental features:\n",
    "\n",
    "1.  **A GPU-accelerated N-dimensional Tensor library**, similar to NumPy but with the ability to run on Graphics Processing Units (GPUs). This allows for massive speedups (10-50x) over running on a CPU.\n",
    "2.  **A deep learning framework** built around an automatic differentiation system called `autograd`. This system allows us to automatically and efficiently calculate gradients, which are the backbone of how neural networks learn.\n",
    "\n",
    "**Official Documentation Links (Your Best Friend!):**\n",
    "* **PyTorch Homepage:** [https://pytorch.org/](https://pytorch.org/)\n",
    "* **Main `torch.Tensor` Docs:** [https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html)\n",
    "* **Autograd Concept Page:** [https://pytorch.org/docs/stable/notes/autograd.html](https://pytorch.org/docs/stable/notes/autograd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is a Tensor?\n",
    "\n",
    "A tensor is a generalization of scalars, vectors, and matrices to an arbitrary number of dimensions. It is the fundamental data structure in PyTorch. Don't be intimidated by the word; it's just a name for a multi-dimensional array.\n",
    "\n",
    "* A **scalar** is a 0-dimensional tensor (a single number).\n",
    "* A **vector** is a 1-dimensional tensor (a 1D array).\n",
    "* A **matrix** is a 2-dimensional tensor (a 2D array).\n",
    "* A 3D tensor can represent an RGB image (height, width, channels).\n",
    "* A 4D tensor can represent a *batch* of RGB images (batch_size, height, width, channels).\n",
    "* A 5D tensor can represent a *batch* of videos (batch_size, sequence_length, height, width, channels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensors\n",
    "\n",
    "You can create tensors in many ways, most of which are identical to their NumPy counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. From Python lists\n",
    "t_list = [[1, 2, 3], [4, 5, 6]]\n",
    "t_from_list = torch.tensor(t_list)\n",
    "print(f\"Tensor from list:\\n{t_from_list}\")\n",
    "print(f\"Shape: {t_from_list.shape}\") # .shape is just like NumPy\n",
    "print(f\"Data type: {t_from_list.dtype}\") # .dtype is just like NumPy\n",
    "\n",
    "# 2. Placeholder tensors (like np.zeros, np.ones)\n",
    "t_zeros = torch.zeros((2, 3))\n",
    "print(f\"\\nZeros tensor:\\n{t_zeros}\")\n",
    "\n",
    "t_ones = torch.ones((3, 2), dtype=torch.float32) # We can specify the data type\n",
    "print(f\"\\nOnes tensor (float32):\\n{t_ones}\")\n",
    "\n",
    "# 3. Tensors with random values (like np.random.rand)\n",
    "t_rand = torch.rand((2, 2)) # Uniform distribution between [0, 1)\n",
    "print(f\"\\nRandom tensor:\\n{t_rand}\")\n",
    "\n",
    "# 4. From ranges (like np.arange)\n",
    "t_range = torch.arange(0, 10, 2) # (start, end, step)\n",
    "print(f\"\\nRange tensor: {t_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Bridge: NumPy vs. PyTorch\n",
    "\n",
    "PyTorch tensors and NumPy arrays are very similar. They share most of their API and can be converted back and forth very efficiently.\n",
    "\n",
    "**This is the most critical concept:** When a NumPy array is converted to a PyTorch tensor (or vice-versa) on the **CPU**, they **share the same underlying memory location**. This means that changing one will change the other!\n",
    "\n",
    "This is done for performance, to avoid making unnecessary copies of large arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. NumPy to PyTorch\n",
    "np_arr = np.array([1, 2, 3, 4])\n",
    "torch_from_np = torch.from_numpy(np_arr)\n",
    "print(f\"NumPy array: {np_arr}\")\n",
    "print(f\"PyTorch tensor: {torch_from_np}\")\n",
    "\n",
    "# 2. PyTorch to NumPy\n",
    "torch_ten = torch.tensor([5, 6, 7, 8])\n",
    "np_from_torch = torch_ten.numpy()\n",
    "print(f\"\\nPyTorch tensor: {torch_ten}\")\n",
    "print(f\"NumPy array: {np_from_torch}\")\n",
    "\n",
    "# 3. Let's prove they share memory!\n",
    "print(\"\\n--- Memory Sharing Demonstration ---\")\n",
    "np_test = np.array([10, 20, 30])\n",
    "torch_test = torch.from_numpy(np_test)\n",
    "print(f\"Original NumPy: {np_test}\")\n",
    "print(f\"Original PyTorch: {torch_test}\")\n",
    "\n",
    "print(\"\\nModifying the NumPy array...\")\n",
    "np_test[0] = 99\n",
    "\n",
    "print(f\"NEW NumPy: {np_test}\")\n",
    "print(f\"NEW PyTorch (changed automatically!): {torch_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Similarity\n",
    "\n",
    "If you know NumPy, you know 90% of PyTorch's tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_a = np.random.rand(3, 3)\n",
    "torch_a = torch.from_numpy(np_a)\n",
    "\n",
    "print(f\"NumPy mean: {np_a.mean()}\")\n",
    "print(f\"PyTorch mean: {torch_a.mean()}\")\n",
    "\n",
    "print(f\"\\nNumPy sum (axis 0): \\n{np_a.sum(axis=0)}\")\n",
    "print(f\"PyTorch sum (dim 0): \\n{torch_a.sum(dim=0)}\")\n",
    "\n",
    "# Matrix Multiplication\n",
    "np_b = np.random.rand(3, 2)\n",
    "torch_b = torch.from_numpy(np_b)\n",
    "\n",
    "print(f\"\\nNumPy matmul:\\n{np_a @ np_b}\")\n",
    "print(f\"PyTorch matmul:\\n{torch_a @ torch_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tensor Reshaping: Flattening\n",
    "\n",
    "A very common operation in deep learning is **flattening** a tensor. Neural networks (especially the first linear layer) often expect a 1D vector of features as input.\n",
    "\n",
    "For example, an image is 2D (or 3D with color channels). To feed it to a simple network, we must \"unroll\" it into a 1D vector.\n",
    "\n",
    "Let's say we have a batch of 10 images, each 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fake batch of 10 images, 28x28 pixels\n",
    "batch_of_images = torch.rand(10, 28, 28)\n",
    "print(f\"Original shape: {batch_of_images.shape}\") # (batch_size, height, width)\n",
    "\n",
    "# Flatten the entire tensor (not very useful)\n",
    "flat_all = batch_of_images.flatten()\n",
    "print(f\"\\nTotally flat shape: {flat_all.shape}\") # 10 * 28 * 28 = 7840\n",
    "\n",
    "# What we usually want is to flatten *each item in the batch*.\n",
    "# We want to keep the batch dimension (10) and unroll the rest (28x28 = 784).\n",
    "# The final shape should be (10, 784).\n",
    "flat_correct = batch_of_images.flatten(start_dim=1)\n",
    "print(f\"\\nFlattened per batch item (start_dim=1): {flat_correct.shape}\")\n",
    "\n",
    "# An older, but very common way to do this is with .view()\n",
    "# We tell PyTorch to keep the first dimension and infer the rest with -1\n",
    "flat_view = batch_of_images.view(10, -1)\n",
    "print(f\"Flattened with .view(10, -1): {flat_view.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Why Do We Need Tensors? The Simple Perceptron\n",
    "\n",
    "This is the core question. We need tensors and matrices for one primary reason: **to perform massive, parallel computations (like matrix multiplication) very, very fast.**\n",
    "\n",
    "The most basic building block of a neural network is a **Linear Layer** (or a perceptron).\n",
    "\n",
    "The math is simple: **`y = Wx + b`**\n",
    "\n",
    "* **`x`**: The input vector (e.g., our flattened 28x28 image, which has 784 features).\n",
    "* **`W`**: The **weights** matrix. This is what the model *learns*. It represents the *strength of the connections* between each input feature and each output neuron.\n",
    "* **`b`**: The **bias** vector. An extra set of learnable parameters to shift the output.\n",
    "* **`y`**: The output vector (e.g., a vector of 10 scores, one for each digit from 0 to 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's model this with tensors:\n",
    "\n",
    "1.  **Input `x`**: A single flattened image. Shape: `(784)`\n",
    "2.  **Weights `W`**: We want 10 output neurons, and we have 784 input features. Shape: `(10, 784)`\n",
    "3.  **Bias `b`**: One bias for each of the 10 output neurons. Shape: `(10)`\n",
    "\n",
    "The operation `Wx` is a **matrix-vector product**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random tensors to simulate the shapes\n",
    "x = torch.rand(784)\n",
    "W = torch.rand(10, 784)\n",
    "b = torch.rand(10)\n",
    "\n",
    "# Perform the linear layer operation\n",
    "y = torch.matmul(W, x) + b\n",
    "# A common shorthand for matmul is '@'\n",
    "# y = W @ x + b\n",
    "\n",
    "print(f\"Shape of W: {W.shape}\")\n",
    "print(f\"Shape of x: {x.shape}\")\n",
    "print(f\"Shape of b: {b.shape}\")\n",
    "print(f\"Shape of output y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The REAL Power: Batch Processing\n",
    "\n",
    "Doing this one image at a time is slow. The *real* reason we use tensors is to process a **batch** of inputs (e.g., 64 images) all at once in a single, parallel operation.\n",
    "\n",
    "1.  **Input `X` (Batch)**: A *matrix* of inputs. Shape: `(64, 784)` (64 images, 784 features each)\n",
    "2.  **Weights `W`**: Same as before. Shape: `(10, 784)`\n",
    "3.  **Bias `b`**: Same as before. Shape: `(10)`\n",
    "\n",
    "Now `(10, 784) @ (64, 784)` won't work. The dimensions don't align.\n",
    "We need to change our operation to: **`Y = X @ W.T + b`** (where `W.T` is the **transpose** of W).\n",
    "\n",
    "* **`X`**: `(64, 784)`\n",
    "* **`W.T`**: `(784, 10)`\n",
    "\n",
    "The **matrix-matrix product** `(64, 784) @ (784, 10)` results in a new matrix of shape `(64, 10)`.\n",
    "\n",
    "This is perfect! It's a batch of 64 output vectors, one for each input image. GPUs are *designed* to do this exact operation (matrix multiplication) extremely fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a batch of 64 images\n",
    "X_batch = torch.rand(64, 784)\n",
    "W = torch.rand(10, 784) # Weights are the same\n",
    "b = torch.rand(10)       # Bias is the same\n",
    "\n",
    "# 2. Perform the batch operation\n",
    "Y_batch = X_batch @ W.T + b\n",
    "\n",
    "print(f\"Shape of X_batch: {X_batch.shape}\")\n",
    "print(f\"Shape of W.T: {W.T.shape}\")\n",
    "print(f\"Shape of b: {b.shape}\")\n",
    "print(f\"\\nShape of output Y_batch: {Y_batch.shape}\")\n",
    "print(\"This is a batch of 64 output vectors, each with 10 scores!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Magic: `autograd` (Automatic Gradients)\n",
    "\n",
    "This is the second, and more magical, reason PyTorch exists. It allows the framework to automatically calculate the gradients (derivatives) of our computations. This is how neural networks learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Real-World Analogy\n",
    "\n",
    "Imagine you are on a hill in the fog and you want to get to the valley (the lowest point). You can't see the valley, but you can feel the **slope** of the ground beneath your feet.\n",
    "\n",
    "* The **hill** represents our error (which we'll call **Loss** in the next lesson). We want to minimize it.\n",
    "* Your **position** on the hill is your model's **Weight/Parameter** (e.g., a number `w`).\n",
    "* The **slope** you feel is the **Gradient**. It's a vector that points in the direction of *steepest ascent* (the fastest way *uphill*).\n",
    "\n",
    "**To get to the valley (minimum error), you just take a small step in the exact opposite direction of the gradient.**\n",
    "\n",
    "**Autograd is the tool that automatically tells us the slope (the gradient) for every single parameter (like `W` and `b`) in our model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it Works: `requires_grad=True`\n",
    "\n",
    "To tell PyTorch that we want to track the computations for a tensor, we set its `requires_grad` flag to `True`. All tensors that are parameters in a neural network will have this set automatically.\n",
    "\n",
    "PyTorch then builds a **computation graph** behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: A Simple Function\n",
    "\n",
    "Let's take a simple function `y = 3x²`. \n",
    "From calculus, we know the derivative (the gradient) of `y` with respect to `x` is `dy/dx = 6x`.\n",
    "\n",
    "Let's see if PyTorch agrees. We'll test it at `x = 4`. The gradient should be `6 * 4 = 24`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create our input tensor 'x' and tell PyTorch to track it\n",
    "x = torch.tensor(4.0, requires_grad=True) # Must be a float to have a gradient\n",
    "\n",
    "# 2. Define our function 'y'\n",
    "y = 3 * x**2\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y}\")\n",
    "\n",
    "# 3. Calculate the gradients\n",
    "# This tells PyTorch to go backward through the computation graph and\n",
    "# calculate the gradient of y with respect to all its dependencies (i.e., x)\n",
    "y.backward()\n",
    "\n",
    "# 4. Check the gradient stored in x.grad\n",
    "print(f\"\\nThe gradient dy/dx at x=4 is: {x.grad}\")\n",
    "\n",
    "assert x.grad == 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: The Computation Graph\n",
    "\n",
    "Let's do a more complex example. This is what happens inside a neural network.\n",
    "\n",
    "`a` and `b` are our parameters (weights). We want to find the gradient of the final scalar output `s` with respect to both `a` and `b`.\n",
    "\n",
    "Computation:\n",
    "1. `c = a * 2`\n",
    "2. `d = b + c`\n",
    "3. `s = d.mean()` (We compute a single summary scalar `s` from the vector `d`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "b = torch.tensor([3.0, 4.0], requires_grad=True)\n",
    "\n",
    "c = a * 2\n",
    "d = b + c\n",
    "s = d.mean() # 's' is our final scalar value\n",
    "\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c: {c}\")\n",
    "print(f\"d: {d}\")\n",
    "print(f\"s: {s}\")\n",
    "\n",
    "# Calculate gradients\n",
    "s.backward()\n",
    "\n",
    "# Check the gradients\n",
    "# The gradient of 's' with respect to 'a' is d(s)/da\n",
    "# The gradient of 's' with respect to 'b' is d(s)/db\n",
    "print(f\"\\nGradient for a: {a.grad}\")\n",
    "print(f\"Gradient for b: {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRITICAL NUANCE 1: Gradient Accumulation\n",
    "\n",
    "By default, PyTorch **accumulates** gradients every time you call `.backward()`. It **adds** the new gradients to the existing ones (it does `x.grad += new_grad`).\n",
    "\n",
    "This is a design choice to support advanced models (like RNNs). For 99% of our use cases, we must **manually set the gradients to zero** after each learning step. If we don't, our gradients will be wrong and the model will not learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# --- First pass ---\n",
    "y1 = x**2  # y = 9, grad = 2x = 6\n",
    "y1.backward()\n",
    "print(f\"After first pass, x.grad = {x.grad}\")\n",
    "\n",
    "# --- Second pass (WITHOUT clearing) ---\n",
    "y2 = x**3  # y = 27, grad = 3x^2 = 27\n",
    "y2.backward()\n",
    "print(f\"After second pass, x.grad = {x.grad}\") # 6 + 27 = 33\n",
    "\n",
    "# This is wrong! The gradient for y2 is 27, not 33. It accumulated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRITICAL NUANCE 2: Clearing Gradients with `.zero_()`\n",
    "\n",
    "To fix this, we must call `.grad.zero_()` on all our parameters *before* we calculate the new gradients (i.e., before the next `.backward()` call).\n",
    "\n",
    "This is why you will *always* see `optimizer.zero_grad()` at the start of a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# --- First pass ---\n",
    "y1 = x**2  # y = 9, grad = 2x = 6\n",
    "y1.backward()\n",
    "print(f\"After first pass, x.grad = {x.grad}\")\n",
    "\n",
    "# --- CLEAR THE GRADIENT ---\n",
    "print(\"\\n--- Clearing gradients ---\")\n",
    "x.grad.zero_() # This is the in-place version of x.grad = torch.zeros_like(x.grad)\n",
    "\n",
    "# --- Second pass (Correct) ---\n",
    "y2 = x**3  # y = 27, grad = 3x^2 = 27\n",
    "y2.backward()\n",
    "print(f\"After second pass, x.grad = {x.grad}\") # Now it's 27, which is correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practice Exercises\n",
    "\n",
    "1.  Create a NumPy array `[10, 20, 30]`. Convert it to a PyTorch tensor. Modify the original NumPy array by changing `10` to `100`. Print the PyTorch tensor. What happened?\n",
    "2.  Create a 2D PyTorch tensor of shape `(4, 5)` with random integers between 0 and 10.\n",
    "3.  Using the tensor from #2, flatten it so that it has a shape of `(4, 5)` and then `(20)`. \n",
    "4.  Calculate the gradient for the function `y = 5x³ + 2x` at the point `x = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise 1 ---\n",
    "print(\"\\n--- Exercise 1 ---\")\n",
    "ex1_np = np.array([10, 20, 30])\n",
    "ex1_torch = torch.from_numpy(ex1_np)\n",
    "ex1_np[0] = 100\n",
    "print(f\"The tensor also changed: {ex1_torch}\") # It shares memory!\n",
    "\n",
    "# --- Exercise 2 ---\n",
    "print(\"\\n--- Exercise 2 ---\")\n",
    "ex2 = torch.randint(0, 10, (4, 5))\n",
    "print(f\"Random 4x5 tensor:\\n{ex2}\")\n",
    "\n",
    "# --- Exercise 3 ---\n",
    "print(\"\\n--- Exercise 3 ---\")\n",
    "ex3_flat_all = ex2.flatten()\n",
    "print(f\"Total flatten shape: {ex3_flat_all.shape}\")\n",
    "\n",
    "# --- Exercise 4 ---\n",
    "print(\"\\n--- Exercise 4 ---\")\n",
    "# y = 5x^3 + 2x\n",
    "# dy/dx = 15x^2 + 2\n",
    "# At x=2, grad = 15*(2^2) + 2 = 15*4 + 2 = 60 + 2 = 62\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = 5*x**3 + 2*x\n",
    "y.backward()\n",
    "print(f\"The gradient at x=2 is: {x.grad}\")\n",
    "assert x.grad == 62"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}