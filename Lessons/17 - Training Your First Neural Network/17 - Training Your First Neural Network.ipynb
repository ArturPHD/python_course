{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 17: Training and Evaluating Your First Neural Network\n",
    "\n",
    "In the last two lessons, we learned the crucial pieces:\n",
    "1.  **Lesson 15:** We learned the 5-step training loop (Forward -> Loss -> Zero Grads -> Backward -> Step).\n",
    "2.  **Lesson 16:** We learned how to load and prepare data with `Dataset` and `DataLoader`.\n",
    "\n",
    "Today, we combine all these pieces. We will define our first true **Neural Network** using `nn.Module` and then train it from start to finish to recognize handwritten digits. This is the complete process from start to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Videos\n",
    "\n",
    "These concepts are the heart of deep learning. Watching visual explanations can be extremely helpful.\n",
    "\n",
    "* **What is a Neural Network? (3Blue1Brown):** A foundational and beautiful explanation of what a neural network *is*. \n",
    "    * [https://www.youtube.com/watch?v=aircAruvnKk](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "\n",
    "* **What is Backpropagation? (3Blue1Brown):** The best explanation of how gradients flow backward through the network. This is the \"how it learns\" part.\n",
    "    * [https://www.youtube.com/watch?v=Ilg3gGewQ5U](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    "\n",
    "* **Cross-Entropy Loss (StatQuest):** A fantastic, simple explanation of *why* we use Cross-Entropy for classification.\n",
    "    * [https://www.youtube.com/watch?v=7q7E91pA9aQ](https://www.youtube.com/watch?v=7q7E91pA9aQ)\n",
    "\n",
    "* **Activation Functions (ReLU):** A quick overview of different activation functions and why ReLU is so popular.\n",
    "    * [https://www.youtube.com/watch?v=68BZlGvl1W4](https://www.youtube.com/watch?v=68BZlGvl1W4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core Concepts: The Model and The Loss\n",
    "\n",
    "Before we write code, let's solidify the key concepts we're using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Perceptron (or a `nn.Linear` Layer)?\n",
    "\n",
    "A perceptron (or a \"Linear Layer\" in PyTorch) is the most basic building block of a neural network. It simply performs the operation: **`output = weights * input + bias`**.\n",
    "\n",
    "* **Analogy:** Think of it as a panel of \"smart knobs\".\n",
    "* The **`weights`** are the knobs. They decide how much *importance* to give to each input feature. If a weight is high, that input feature strongly affects the output. If it's zero, that input is ignored.\n",
    "* The **`bias`** is a \"starting offset\" knob. It helps shift the entire output up or down, regardless of the input. \n",
    "\n",
    "The model \"learns\" by finding the perfect settings for all these knobs (`weights` and `biases`) to get the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an Activation Function (like `nn.ReLU`)?\n",
    "\n",
    "If you just stack a bunch of Linear layers, you can only learn simple lines. To learn complex patterns (like the shape of a \"3\"), we need to introduce **non-linearity**.\n",
    "\n",
    "An activation function is a simple \"switch\" applied after each layer. The most popular one is **ReLU (Rectified Linear Unit)**. \n",
    "\n",
    "Its logic is \"f(x) = max(0, x)\".\n",
    "* If the input is positive (e.g., 5.0), it passes it through (output is 5.0).\n",
    "* If the input is negative (e.g., -3.0), it clips it to **zero**.\n",
    "\n",
    "This simple \"bend\" at zero is enough to let our network learn curves, corners, and all sorts of complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is One-Hot Encoding?\n",
    "\n",
    "Our labels are numbers like `[5, 0, 4, 1, 9, ...]`. We have a problem: does the model think that the digit `9` is \"better\" or \"more\" than the digit `1`? This is a false relationship that could confuse the model.\n",
    "\n",
    "We need to treat each digit as a separate, independent category. We do this with **One-Hot Encoding**.\n",
    "\n",
    "We create a vector of all possible classes (10 in our case, for digits 0-9) and set the correct class to `1` and all others to `0`.\n",
    "\n",
    "* `3` becomes `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]` (index 3 is \"on\")\n",
    "* `9` becomes `[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]` (index 9 is \"on\")\n",
    "* `0` becomes `[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]` (index 0 is \"on\")\n",
    "\n",
    "The model's final output (a vector of 10 numbers) will be its attempt to recreate this one-hot vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Cross-Entropy Loss?\n",
    "\n",
    "This is the loss function we use for classification. It's the best way to measure \"how wrong\" our model is.\n",
    "\n",
    "**Analogy: The \"Confidence Penalty\"**\n",
    "\n",
    "Imagine the correct answer is `3`. The one-hot vector is `[0, 0, 0, 1, 0, ...]`. \n",
    "Our model outputs **logits** (raw scores), which we can think of as *confidence scores*.\n",
    "\n",
    "* **Good Prediction:** `[-1.2, 0.5, 0.9, 8.5, 1.1, ...]`\n",
    "    * The model is *very confident* (8.5) that the answer is `3`. It is correct and confident. **LOW LOSS**.\n",
    "\n",
    "* **Unsure Prediction:** `[0.1, 0.2, 0.3, 0.5, 0.1, ...]`\n",
    "    * The model's highest score is for `3`, so it's *correct*, but it's not *confident* (0.5 is not much higher than 0.3). **MEDIUM LOSS**.\n",
    "\n",
    "* **Bad Prediction:** `[9.0, -1.5, 0.1, 0.2, -0.5, ...]`\n",
    "    * The model is *very confident* (9.0) that the answer is `0`. It is confident and **WRONG**. **HIGH LOSS**.\n",
    "\n",
    "**Cross-Entropy Loss** perfectly captures this. It *punishes* the model heavily for being confident and wrong. This forces the model to become *both* accurate and confident in its correct predictions.\n",
    "\n",
    "**PyTorch Nuance:** `nn.CrossEntropyLoss` is a 2-in-1 function. It automatically applies a \"Softmax\" (which turns logits into probabilities) and then calculates the loss. It's very efficient. It also means we pass in the *raw class index* (e.g., `3`) as the target, not the one-hot vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup: Imports and Data Loading\n",
    "\n",
    "Let's import everything we need and use our `data_loader.py` file to get the data."
   ]
  },
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "data_loader_path = os.path.join(parent_dir, '16 - Datasets')\n",
    "\n",
    "if data_loader_path not in sys.path:\n",
    "    sys.path.append(data_loader_path)\n",
    "\n",
    "from data_loader import get_mnist_loaders\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader, val_loader, test_loader = get_mnist_loaders(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Neural Network (`nn.Module`)\n",
    "\n",
    "This is where we build our model's architecture. We create a class that inherits from `nn.Module`. This is the standard, most flexible way to build models in PyTorch.\n",
    "\n",
    "It has two main parts:\n",
    "1.  **`__init__(self)`**: The constructor. This is where we *define and initialize* all the layers our model will use (e.g., linear layers, flatten, etc.).\n",
    "2.  **`forward(self, x)`**: This function defines the *data flow*. It describes how an input `x` moves through the layers we defined in `__init__` to produce an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        \n",
    "        # This layer flattens the [1, 28, 28] image into a [784] vector\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # We define our network as a sequence of layers\n",
    "        self.layers = nn.Sequential(\n",
    "            # 1st Layer: 784 inputs -> 128 outputs\n",
    "            nn.Linear(input_size, hidden_size1),\n",
    "            nn.ReLU(), # Activation Function\n",
    "            \n",
    "            # 2nd Layer: 128 inputs -> 64 outputs\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.ReLU(), # Activation Function\n",
    "            \n",
    "            # Output Layer: 64 inputs -> 10 outputs (logits)\n",
    "            nn.Linear(hidden_size2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward pass defines how data flows through the layers\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "# --- Define our constants ---\n",
    "INPUT_SIZE = 784  # 28*28 pixels\n",
    "HIDDEN_SIZE_1 = 128\n",
    "HIDDEN_SIZE_2 = 64\n",
    "NUM_CLASSES = 10   # Digits 0-9\n",
    "\n",
    "# --- Instantiate the model ---\n",
    "model = SimpleNN(INPUT_SIZE, HIDDEN_SIZE_1, HIDDEN_SIZE_2, NUM_CLASSES)\n",
    "print(\"Our Model Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Full Training Loop (Run 1: Normal Training)\n",
    "\n",
    "This is the main event! We will now write the full 5-step training loop and add a validation loop inside it to check our progress. An **epoch** is one full pass through the *entire* training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# --- Define Loss and Optimizer ---\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- We will log our progress here ---\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(\"--- Starting Training (10 Epochs) ---\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # --- TRAINING LOOP ---\n",
    "    model.train() # Set the model to training mode (e.g., activates dropout)\n",
    "    current_train_loss = 0.0\n",
    "    for images, labels in train_loader: \n",
    "        # 1. Forward Pass: Get model's predictions\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # 2. Calculate Loss: Measure how wrong the predictions are\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # 3. Zero Gradients: Reset gradients from previous loop\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. Backward Pass: Calculate gradients for all parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update Parameters: Take a step using the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        current_train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    epoch_train_loss = current_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # --- VALIDATION LOOP ---\n",
    "    model.eval() # Set the model to evaluation mode (e.g., deactivates dropout)\n",
    "    current_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculation for efficiency\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            current_val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1) # Get the index of the max score\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average validation loss and accuracy for the epoch\n",
    "    epoch_val_loss = current_val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    epoch_val_acc = 100 * correct / total\n",
    "    val_accuracies.append(epoch_val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.2f}%\")\n",
    "\n",
    "print(\"--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plotting and Final Evaluation\n",
    "\n",
    "Now we'll use Matplotlib to visualize the data we logged. This is the *most important* way to see if our model is learning correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# Plot 1: Training & Validation Loss\n",
    "ax1.plot(train_losses, label='Training Loss')\n",
    "ax1.plot(val_losses, label='Validation Loss')\n",
    "ax1.set_title('Training vs. Validation Loss (10 Epochs)')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot 2: Validation Accuracy\n",
    "ax2.plot(val_accuracies, label='Validation Accuracy', color='green')\n",
    "ax2.set_title('Validation Accuracy (10 Epochs)')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation (The \"Final Exam\")\n",
    "\n",
    "Our model looks good! The validation loss is decreasing and the accuracy is increasing. Now it's time for the \"final exam\" - running our trained model on the **Test Set** (the data it has *never* seen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "final_accuracy = 100 * correct / total\n",
    "print(f\"\\n--- FINAL TEST ACCURACY ---\")\n",
    "print(f\"Accuracy on 10,000 test images: {final_accuracy:.2f} % \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overfitting and Early Stopping\n",
    "\n",
    "What happens if we let the model train for much longer? Let's re-initialize our model and train it for 30 epochs to see.\n",
    "\n",
    "**Overfitting** is when the model stops learning the *general patterns* (e.g., what a \"7\" looks like) and starts *memorizing* the specific quirks of the training data (e.g., \"this specific pixel on this specific image is always off\").\n",
    "\n",
    "When this happens, its performance on new, unseen data (the validation set) gets *worse*. The training loss will continue to go down, but the validation loss will start to go **up**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Re-initialize model and optimizer ---\n",
    "model_long_run = SimpleNN(INPUT_SIZE, HIDDEN_SIZE_1, HIDDEN_SIZE_2, NUM_CLASSES)\n",
    "optimizer_long_run = optim.SGD(model_long_run.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn_long_run = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- New lists for logging ---\n",
    "long_train_losses = []\n",
    "long_val_losses = []\n",
    "long_val_accuracies = []\n",
    "\n",
    "NUM_EPOCHS_LONG = 30\n",
    "\n",
    "print(\"--- Starting LONG Training (30 Epochs) to demonstrate overfitting ---\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_LONG):\n",
    "    model_long_run.train()\n",
    "    current_train_loss = 0.0\n",
    "    for images, labels in train_loader: \n",
    "        outputs = model_long_run(images)\n",
    "        loss = loss_fn_long_run(outputs, labels)\n",
    "        optimizer_long_run.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_long_run.step()\n",
    "        current_train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_train_loss = current_train_loss / len(train_loader.dataset)\n",
    "    long_train_losses.append(epoch_train_loss)\n",
    "\n",
    "    model_long_run.eval()\n",
    "    current_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model_long_run(images)\n",
    "            loss = loss_fn_long_run(outputs, labels)\n",
    "            current_val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_val_loss = current_val_loss / len(val_loader.dataset)\n",
    "    long_val_losses.append(epoch_val_loss)\n",
    "    epoch_val_acc = 100 * correct / total\n",
    "    long_val_accuracies.append(epoch_val_acc)\n",
    "    \n",
    "    # We log everything, even if the results don't change much\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS_LONG} | Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.2f}%\")\n",
    "\n",
    "print(\"--- Long Training Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Overfitting Example\n",
    "\n",
    "Now let's plot the 30-epoch run. Pay close attention to the loss graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# Plot 1: Training & Validation Loss (30 Epochs)\n",
    "ax1.plot(long_train_losses, label='Training Loss')\n",
    "ax1.plot(long_val_losses, label='Validation Loss')\n",
    "ax1.set_title('Training vs. Validation Loss (30 Epochs)')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot 2: Validation Accuracy (30 Epochs)\n",
    "ax2.plot(long_val_accuracies, label='Validation Accuracy', color='green')\n",
    "ax2.set_title('Validation Accuracy (30 Epochs)')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Plots\n",
    "\n",
    "Look at the 'Training vs. Validation Loss' plot. You can clearly see:\n",
    "\n",
    "1.  **Training Loss** (blue line) consistently goes down. The model is getting better and better at matching the training data.\n",
    "2.  **Validation Loss** (orange line) goes down for the first ~10-15 epochs, but then it **starts to rise again**. \n",
    "\n",
    "**This inflection point is where overfitting begins.** The model is now learning the *noise* of the training set, not the *pattern* of digits. Its performance on unseen validation data gets worse.\n",
    "\n",
    "**Early Stopping** is the simple technique of monitoring the validation loss and stopping the training (and saving the model) at the epoch where the validation loss was at its **minimum** (around epoch 10-15 in this graph), not at the end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}