{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 15: Your First PyTorch Model - Linear Regression\n",
    "\n",
    "Welcome! In the last lesson, we learned about Tensors (the data) and Autograd (the gradient calculator). Today, we will combine these concepts to build our very first functional model that can *learn* from data.\n",
    "\n",
    "We will build a **Linear Regression** model. This is the \"Hello, World!\" of machine learning. \n",
    "\n",
    "**Our Goal:** Find the line of best fit for a set of data points. We are trying to teach the computer to find the values for `w` (weight) and `b` (bias) in the famous equation: `y = w * x + b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 5 Steps of Deep Learning Training\n",
    "\n",
    "To train *any* model in PyTorch, we will always follow these steps. We will repeat this pattern in every lesson from now on.\n",
    "\n",
    "**The Training Loop:**\n",
    "1.  **Forward Pass:** Our model makes a prediction.\n",
    "2.  **Calculate Loss:** We measure *how wrong* the prediction is.\n",
    "3.  **Zero Gradients:** We reset the gradient so they don't add up from previous steps.\n",
    "4.  **Backward Pass:** PyTorch calculates the gradients (the \"slope\" of our error) for every parameter.\n",
    "5.  **Update Parameters:** We use an optimizer to take a small step in the correct direction to reduce the error.\n",
    "\n",
    "Let's learn this by doing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # nn is PyTorch's module for building neural networks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the Data\n",
    "\n",
    "In a real project, we would load data. For this first example, let's *create* some data. We'll start with a known line, `y = 2x + 1`, and add some random \"noise\" to it. \n",
    "\n",
    "Our model's job will be to discover that `w` is close to `2` and `b` is close to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 100 data points\n",
    "X = torch.randn(100, 1) * 10 # 100 rows, 1 column. Multiplied by 10 to spread the data out\n",
    "\n",
    "# Our \"true\" line is y = 2x + 1\n",
    "# We add random noise to make it a realistic problem\n",
    "y = 2 * X + 1 + torch.randn(100, 1) * 2 # Add noise with a standard deviation of 2\n",
    "\n",
    "# Let's visualize our data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X.numpy(), y.numpy())\n",
    "plt.title('Our Synthetic Data (What the model sees)')\n",
    "plt.xlabel('X (Input Feature)')\n",
    "plt.ylabel('y (Target Value)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The \"Manual\" Way (Building from scratch)\n",
    "\n",
    "Let's build the model by manually defining our parameters `w` and `b`. This is the best way to understand what's happening under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initialize Parameters\n",
    "\n",
    "We need to start with a guess for `w` and `b`. Let's guess `w = 0.0` and `b = 0.0`. \n",
    "\n",
    "Crucially, we must set `requires_grad=True`. This tells PyTorch: \"Track every calculation that uses these tensors, because we will need to calculate gradients for them later.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Model, Loss, and Optimizer\n",
    "\n",
    "**1. The Model (Forward Pass):** This is just a function that implements our equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return w * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The Loss Function (Criterion):**\n",
    "\n",
    "How do we measure \"how wrong\" our predictions are? We use a **Loss Function**.\n",
    "\n",
    "**Analogy:** Imagine you're playing a guessing game. You guess a number, and your friend says \"you're off by 10\" or \"you're off by 2\". That \"you're off by\" number is the **loss**. A high number means you're very wrong. A low number (like 0) means you're right.\n",
    "\n",
    "For regression, the most common loss function is **Mean Squared Error (MSE)**.\n",
    "\n",
    "**MSE = (1/N) * sum( (prediction - actual)Â² )**\n",
    "\n",
    "In simple terms: *\"For every data point, find the difference between our guess and the real answer. Square that difference (to make it positive and to punish big mistakes more). Then, take the average of all those squared differences.\"*\n",
    "\n",
    "Luckily, PyTorch has this built-in: `nn.MSELoss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. The Optimizer:**\n",
    "\n",
    "If the Loss Function is the *measurement* of how wrong we are, and the Gradient (from Lesson 14) is the *direction* of the mistake, the **Optimizer** is the tool that *takes the step* to fix the mistake.\n",
    "\n",
    "We will use the simplest and most famous optimizer: **Stochastic Gradient Descent (SGD)**.\n",
    "\n",
    "We need to tell the optimizer two things:\n",
    "1.  **What parameters to update?** In our case, `w` and `b`.\n",
    "2.  **How big of a step to take?** This is the **Learning Rate (`lr`)**. It's the most important knob to tune. \n",
    "    * A **high `lr`** is like taking giant leaps down the hill. You might be fast, but you might leap right over the valley.\n",
    "    * A **low `lr`** is like taking tiny baby steps. You'll be very precise, but it might take forever to get to the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 \n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: The Training Loop\n",
    "\n",
    "Now we'll put all 5 steps together and repeat them many times. One full pass through our data is called one **epoch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1. Forward Pass: Make a prediction\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # 2. Calculate Loss: How wrong are we?\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    # 3. Zero Gradients: Reset gradients from previous loop\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backward Pass: Calculate gradients (the magic!)\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Update Parameters: Take a step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(f\"\\nTraining finished!\\nLearned w: {w.item():.4f}, Learned b: {b.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Visualize the Result\n",
    "\n",
    "Our model learned that `w` is around `2.0` and `b` is around `1.0`. Let's plot the line our model learned against the original data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model's final predictions\n",
    "predicted = forward(X).detach() # .detach() removes it from the autograd graph\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X.numpy(), y.numpy(), label='Original Data')\n",
    "plt.plot(X.numpy(), predicted.numpy(), 'r-', label='Fitted Line')\n",
    "plt.title('Linear Regression - Manual Model')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The \"PyTorch Way\" (Using `nn.Module`)\n",
    "\n",
    "Manually tracking `w` and `b` is great for learning, but it's not practical for large models with millions of parameters. \n",
    "\n",
    "The \"proper\" way to build models in PyTorch is to create a class that inherits from `nn.Module`. This class will hold all our layers and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 (Revisited): Define the Model as a Class\n",
    "\n",
    "Instead of `w` and `b`, we will use a built-in **Linear Layer**: `nn.Linear(in_features, out_features)`.\n",
    "\n",
    "* `in_features=1`: Our `X` data has 1 feature.\n",
    "* `out_features=1`: We want 1 output value (`y_pred`).\n",
    "\n",
    "This `nn.Linear` layer automatically creates and manages the `w` and `b` parameters for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # Define the layers\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define the flow of data\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearRegressionModel(input_dim=1, output_dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 (Revisited): Define Loss and Optimizer\n",
    "\n",
    "The Loss Function is the same. The Optimizer is *almost* the same, but instead of passing `[w, b]`, we can just ask the model for all its parameters automatically using `model.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 (Revisited): The Training Loop\n",
    "\n",
    "The loop is almost identical. The only change is that we call `model(X)` instead of our old `forward(X)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1. Forward Pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # 2. Calculate Loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    # 3. Zero Gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backward Pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Update Parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"\\nTraining finished!\\n\")\n",
    "\n",
    "# You can inspect the learned parameters inside the model\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 (Revisited): Visualize the Result\n",
    "\n",
    "The result should be identical to our manual model, proving that `nn.Linear` was just doing the same `w*x + b` operation all along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model's final predictions\n",
    "predicted = model(X).detach()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X.numpy(), y.numpy(), label='Original Data')\n",
    "plt.plot(X.numpy(), predicted.numpy(), 'r-', label='Fitted Line (nn.Module)')\n",
    "plt.title('Linear Regression - PyTorch nn.Module')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You have just built and trained your first machine learning model from scratch. You learned the 5-step training loop, which is the foundation for all deep learning, from simple linear regression to giant models like ChatGPT.\n",
    "\n",
    "In the next lesson, we will use these exact same steps to build a true **neural network** to solve a more exciting problem: classifying images of handwritten digits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}