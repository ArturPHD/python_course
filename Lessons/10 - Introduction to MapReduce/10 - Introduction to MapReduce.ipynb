{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 10: Introduction to MapReduce\n",
    "\n",
    "Welcome to the second part of our course! We're moving from Python fundamentals to the concepts behind processing massive datasets. Today, we'll explore **MapReduce**, a programming model that revolutionized how we think about and work with \"Big Data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem: What is \"Big Data\"?\n",
    "\n",
    "Imagine you have a log file from a popular website. It's 500 gigabytes in size. You need to count how many times each unique visitor IP address appears in the file. \n",
    "\n",
    "You can't just load this file into memory on your laptopâ€”it won't fit. Reading it line-by-line might work, but it would take an extremely long time on a single machine.\n",
    "\n",
    "This is the core problem of Big Data: **how do you process datasets that are too large to fit or be processed on a single computer in a reasonable amount of time?**\n",
    "\n",
    "The solution is to use **distributed computing**: splitting the problem across many computers (a **cluster**) and having them work in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A Brief History of MapReduce\n",
    "\n",
    "In the early 2000s, Google was facing this exact problem on an unprecedented scale. They needed to process the entire internet to build their search index. To solve this, two Google engineers, Jeffrey Dean and Sanjay Ghemawat, developed a new programming model and published a now-famous paper in 2004 called **\"MapReduce: Simplified Data Processing on Large Clusters\"**.\n",
    "\n",
    "The idea was brilliant: create a simple abstraction that allows programmers to focus on their specific data processing logic, while a powerful framework handles all the complex details of distributing the work, managing failures, and coordinating hundreds or thousands of machines.\n",
    "\n",
    "This paper inspired the open-source community, leading to the creation of **Apache Hadoop**, which became the standard for big data processing for many years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recommended Videos\n",
    "\n",
    "Before we dive into the technical details, these videos provide excellent visual explanations of the concept:\n",
    "\n",
    "* **Hadoop In 5 Minutes | What Is Hadoop? | Introduction To Hadoop | Hadoop Explained |Simplilearn**: [https://www.youtube.com/watch?v=cHGaQz0E7AU](https://www.youtube.com/watch?v=cHGaQz0E7AU) (A great, simple animated overview).\n",
    "* **Map Reduce explained with example**: [https://www.youtube.com/watch?v=aReuLtY0YMI](https://www.youtube.com/watch?v=aReuLtY0YMI) (Goes through the Word Count example step-by-step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Core Concept: Map, Shuffle, and Reduce\n",
    "\n",
    "MapReduce breaks a large task into three main phases. Let's use the classic example: **counting word frequencies** in a collection of documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: The `Map` Phase\n",
    "\n",
    "* **Goal**: Take the raw input data and transform it into intermediate `(key, value)` pairs.\n",
    "* **Process**: The large input data is split into smaller chunks. Each chunk is sent to a **Mapper** task. The mapper's job is to read its chunk of data and apply a function (the `map` function) to each record.\n",
    "* **Example (Word Count)**: The `map` function reads a line of text, splits it into words, and for each word, it emits a pair of `(word, 1)`. The `1` signifies that we've seen this word once.\n",
    "\n",
    "```\n",
    "Input to Mapper 1: \"The quick brown fox\"\n",
    "Output of Mapper 1: [(The, 1), (quick, 1), (brown, 1), (fox, 1)]\n",
    "\n",
    "Input to Mapper 2: \"The lazy brown dog\"\n",
    "Output of Mapper 2: [(The, 1), (lazy, 1), (brown, 1), (dog, 1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: The `Shuffle & Sort` Phase\n",
    "\n",
    "* **Goal**: Group all intermediate values by their key.\n",
    "* **Process**: This is the **magic of the framework**. It collects the output from all mappers, sorts it by key, and groups the values for each key together. This prepares the data for the final phase.\n",
    "* **Example (Word Count)**: The framework sees `(The, 1)` from Mapper 1 and `(The, 1)` from Mapper 2. It groups them.\n",
    "\n",
    "```\n",
    "Input to Shuffle: All the outputs from all mappers.\n",
    "Output of Shuffle (sent to Reducers):\n",
    "(The, [1, 1])\n",
    "(quick, [1])\n",
    "(brown, [1, 1])\n",
    "(fox, [1])\n",
    "(lazy, [1])\n",
    "(dog, [1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: The `Reduce` Phase\n",
    "\n",
    "* **Goal**: Aggregate, summarize, or process the grouped values for each key to produce the final result.\n",
    "* **Process**: A **Reducer** task is created for each unique key (or a group of keys). It receives a key and the list of all its associated values.\n",
    "* **Example (Word Count)**: The `reduce` function for the key `\"The\"` receives `(The, [1, 1])`. Its job is to sum the list of values (1 + 1 = 2) and emit the final result.\n",
    "\n",
    "```\n",
    "Input to Reducer 1: (The, [1, 1])\n",
    "Output of Reducer 1: (The, 2)\n",
    "\n",
    "Input to Reducer 2: (brown, [1, 1])\n",
    "Output of Reducer 2: (brown, 2)\n",
    "\n",
    "Input to Reducer 3: (quick, [1])\n",
    "Output of Reducer 3: (quick, 1)\n",
    "...and so on.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What YOU Write vs. What the Framework Does\n",
    "\n",
    "This is the most important takeaway. The framework abstracts away 99% of the complexity of distributed computing.\n",
    "\n",
    "| Your Responsibility (The Programmer) | The Framework's Responsibility (Hadoop/Spark) |\n",
    "| :--- | :--- |\n",
    "| **1. Write the `map` function logic.** | **1. Parallelization**: Running mappers and reducers on many machines. |\n",
    "| **2. Write the `reduce` function logic.** | **2. Data Distribution**: Splitting input and sending it to mappers. |\n",
    "| **3. Configure the job.** (e.g., input/output files) | **3. The ENTIRE Shuffle & Sort stage.** |\n",
    "| | **4. Fault Tolerance**: Restarting tasks if a machine fails. |\n",
    "| | **5. Communication**: Handling all network traffic between nodes. |\n",
    "| | **6. Monitoring & Logging**: Reporting the job's progress. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. How is it Used in the Real World?\n",
    "\n",
    "### Systems and Frameworks\n",
    "* **Apache Hadoop**: The original, open-source implementation of MapReduce. It's very robust and powerful but can be complex to set up and is relatively slow because it writes intermediate results to disk.\n",
    "* **Apache Spark**: A more modern, faster, and more flexible successor. Spark can run MapReduce-style jobs, but it does so much more efficiently, often performing operations in-memory, which makes it 10-100x faster than Hadoop for many tasks. Today, Spark is far more common for new projects than Hadoop MapReduce.\n",
    "\n",
    "### How to Run a Job\n",
    "\n",
    "**On your local computer (Simulation for learning):**\n",
    "In our next lesson, we will simulate the MapReduce flow. We will write our own `map` and `reduce` functions in Python and use standard loops and dictionaries to imitate the \"Shuffle & Sort\" phase. This is perfect for understanding the logic without needing a complex cluster setup.\n",
    "\n",
    "**On a real cluster (Production environment):**\n",
    "1.  **Package the code**: Your `map` and `reduce` functions are packaged into a file (e.g., a Python script or a Java JAR file).\n",
    "2.  **Upload data**: The large input dataset is uploaded to a distributed file system (like HDFS - Hadoop Distributed File System).\n",
    "3.  **Submit the job**: You run a command from a terminal, telling the cluster manager (like YARN - Yet Another Resource Negotiator) to start your MapReduce job.\n",
    "4.  **Execution**: The cluster manager allocates resources (CPU, RAM) on different machines, sends your code and the data chunks to them, and monitors the execution.\n",
    "5.  **Get results**: The final output from the reducers is written to another folder in the distributed file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practice: \"On Paper\" Word Count\n",
    "\n",
    "Let's solidify the concept. Manually trace the MapReduce process for the following input sentences. Assume we have two mappers.\n",
    "\n",
    "**Input for Mapper 1:** `A big big car`\n",
    "**Input for Mapper 2:** `A small car`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Map Phase\n",
    "* **Mapper 1 Output:** ?\n",
    "* **Mapper 2 Output:** ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Shuffle & Sort Phase\n",
    "* **Grouped Output:** ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Reduce Phase\n",
    "* **Final Output:** ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**Step 1: Map Phase**\n",
    "* **Mapper 1 Output:** `[(A, 1), (big, 1), (big, 1), (car, 1)]`\n",
    "* **Mapper 2 Output:** `[(A, 1), (small, 1), (car, 1)]`\n",
    "\n",
    "**Step 2: Shuffle & Sort Phase**\n",
    "* **Grouped Output:** \n",
    "    * `A: [1, 1]`\n",
    "    * `big: [1, 1]`\n",
    "    * `car: [1, 1]`\n",
    "    * `small: [1]`\n",
    "\n",
    "**Step 3: Reduce Phase**\n",
    "* **Final Output:**\n",
    "    * `(A, 2)`\n",
    "    * `(big, 2)`\n",
    "    * `(car, 2)`\n",
    "    * `(small, 1)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
